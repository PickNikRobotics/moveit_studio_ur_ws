<root>
    <TreeNodesModel>
        <!-- Gibson ML behaviors -->
        <Action ID="GetCuboidFromDetections">
            <metadata subcategory="Perception"/>
            <description>
                <p>
                    Tries to find a Cuboid using the object detections result.
                </p>
            </description>
            <input_port name="object_name" default="picknik_cube">Class name of the object as detected by the network.</input_port>
            <input_port name="object_detections_goal" default="{object_detections_goal}">Name of the object detections goal.</input_port>
            <input_port name="object_detections_result" default="{object_detections_result}">Name of the object detections result.</input_port>
            <input_port name="fit_object_3d_service_name" default="fit_object_3d">Name of the fit_object_3d service.</input_port>
            <output_port name="pose" default="{pose}">Pose representing the cuboid.</output_port>
        </Action>
        <Action ID="GetLeverHandleFromDetections">
            <metadata subcategory="Perception"/>
            <description>
                <p>
                    Tries to find a Lever Handle using the object detections result.
                </p>
            </description>
            <input_port name="object_detections_goal" default="{object_detections_goal}">Name of the object detections goal.</input_port>
            <input_port name="object_detections_result" default="{object_detections_result}">Name of the object detections result.</input_port>
            <input_port name="fit_object_3d_service_name" default="fit_object_3d">Name of the fit_object_3d service.</input_port>
            <output_port name="handle_pose" default="{handle_pose}">Pose representing the handle.</output_port>
            <output_port name="length" default="{length}">length of the handle, from tip to pivot.</output_port>
            <output_port name="height" default="{height}">height of the handle, from door surface to handle surface.</output_port>
        </Action>
        <Action ID="GetButtonFromDetections">
            <metadata subcategory="Perception"/>
            <description>
                <p>
                    Tries to find a Button using the object detections result.
                </p>
            </description>
            <input_port name="object_name" default="button">Class name of the object as detected by the network.</input_port>
            <input_port name="object_detections_goal" default="{object_detections_goal}">Name of the object detections goal.</input_port>
            <input_port name="object_detections_result" default="{object_detections_result}">Name of the object detections result.</input_port>
            <input_port name="fit_object_3d_service_name" default="fit_object_3d">Name of the fit_object_3d service.</input_port>
            <output_port name="button_pose" default="{button_pose}">Pose representing the button.</output_port>
        </Action>
        <Action ID="GetCabinetDoorFromDetections">
            <metadata subcategory="Perception"/>
            <description>
                <p>
                   Tries to find a Cabinet Door using the object detections result.
                </p>
            </description>
            <input_port name="object_detections_goal" default="{object_detections_goal}">Name of the object detections goal.</input_port>
            <input_port name="object_detections_result" default="{object_detections_result}">Name of the object detections result.</input_port>
            <input_port name="fit_object_3d_service_name" default="fit_object_3d">Name of the fit_object_3d service.</input_port>
            <output_port name="grasp_pose" default="{grasp_pose}">Pose representing the grasp point.</output_port>
            <output_port name="hinge_axis_pose_start" default="{hinge_axis_pose_start}">Pose marking the start of the hinge axis.</output_port>
            <output_port name="hinge_axis_pose_end" default="{hinge_axis_pose_end}">Pose marking the end of the hinge axis.</output_port>
        </Action>
        <Action ID="DetectObjects">
            <metadata subcategory="Perception"/>
            <description>
                <p>
                    Detects objects using a trained model.
                </p>
            </description>
            <input_port name="rgb_image_topic" default="/wrist_mounted_camera/color/image_raw">Name of the RGB image topic.</input_port>
            <input_port name="depth_image_topic" default="/wrist_mounted_camera/depth/image_rect_raw">Name of the depth image topic.</input_port>
            <input_port name="camera_info_topic" default="/wrist_mounted_camera/color/camera_info">Name of the camera info topic.</input_port>
            <input_port name="detect_objects_action_name" default="/detect_objects">Name of the action advertised by an object detection server.</input_port>
            <output_port name="object_detections_goal" default="{object_detections_goal}">Name of the object detections goal.</output_port>
            <output_port name="object_detections_result" default="{object_detections_result}">Name of the object detections result.</output_port>
            <output_port name="min_probability" default="0.1">Filter out detections with confidence below this number. Must be in the range [0,1].</output_port>
        </Action>

        <!-- These two behaviors have been added to support Visual Servoing, requires the visual servoing plugin! -->
        <Action ID="VisualServo">
            <description>
                <p>
                    PROTOTYPE ONLY. USE WITH CAUTION.
                </p>
                <p>
                    Sample behavior for servoing towards a streaming pose.
                    Ingests a streaming goal pose, and implements a basic proportional controller around MoveIt Servo to jog the end effector towards the pose.
                    If the goal pose is moving with constant velocity, can optionally include a velocity feedforward term to the controller to drive steady state error to 0.
                </p>
            </description>
            <input_port name="parameters" default="{parameters}">Parameters for the visual servo bt node.</input_port>
            <input_port name="controller_name" default="streaming_controller">Name of the controller activate.</input_port>
            <input_port name="goal_pose_topic" default="/visual_servo_goal_pose_filtered">The streaming goal pose topic for the end effector.</input_port>
            <input_port name="pose_error_topic" default="/visual_servo_pose_error">The error between the gripper and the goal pose will be published to this topic.</input_port>
        </Action>
        <Action ID="PoseStreamFilter">
            <description>
                <p>
                    Ingests a PoseStamped message, applies a filter, and publishes the result.
                </p>
            </description>
            <input_port name="input_pose_topic" default="/visual_servo_goal_pose">PoseStamped topic to filter.</input_port>
            <input_port name="output_pose_topic" default="/visual_servo_goal_pose_filtered">PoseStamped topic to publish filtered data to.</input_port>
            <input_port name="filter_coefficient" default="0.1">Filter coefficient for the smoothing filter.</input_port>
        </Action>
        <Action ID="GraspPoseStreamer">
            <description>
                <p>
                    PROTOTYPE ONLY. USE WITH CAUTION.
                </p>
                <p>
                    Sample behavior for use in visual servoing objectives.
                    Ingests published AprilTag poses relative to the camera, and determines a fixed pose for grasping an object relative to the detected tag.
                    Once the measured error published by the servo node drops below the specified threshold, this behavior will return success.
                </p>
                <p>
                    This behavior could be replaced by a more intelligent system that uses transforms or other image features to determine an ideal grasp pose on the fly.
                </p>
            </description>
            <input_port name="target_frame_id" default="">The frame id for the detected object.</input_port>
            <input_port name="detections_streaming_topic" default="/apriltag_detections">Object detections topic name, generally from the AprilTag.</input_port>
            <input_port name="goal_pose_streaming_topic" default="/visual_servo_goal_pose">Topic on which to publish the goal pose, generally subscribed to by the visual servo.</input_port>
            <input_port name="pose_error_topic" default="/visual_servo_pose_error">Pose error topic to subscribe to, generally published from visual servo.</input_port>
            <input_port name="goal_frame_id" default="base_link">Target frame to transform detections into.</input_port>
            <input_port name="goal_linear_threshold" default="0.03">Maximum error to say the visual servo has reached the linear goal with the gripper.</input_port>
            <input_port name="goal_angular_threshold" default="0.05">Maximum error to say the visual servo has reached the angular goal with the gripper.</input_port>
            <input_port name="goal_pose_translation_x" default="0">X offset  between the AprilTag and the grasp pose.</input_port>
            <input_port name="goal_pose_translation_y" default="0">Y offset between the AprilTag and the grasp pose.</input_port>
            <input_port name="goal_pose_translation_z" default="0.1">Z offset between the AprilTag and the grasp pose.</input_port>
            <input_port name="goal_pose_orientation_x" default="1.0">X-component of the quaternion describing the orientation of the grasp pose relative to the AprilTag frame.</input_port>
            <input_port name="goal_pose_orientation_y" default="0.0">Y-component of the quaternion describing the orientation of the grasp pose relative to the AprilTag frame.</input_port>
            <input_port name="goal_pose_orientation_z" default="0.0">Z-component of the quaternion describing the orientation of the grasp pose relative to the AprilTag frame.</input_port>
            <input_port name="goal_pose_orientation_w" default="0.0">W-component of the quaternion describing the orientation of the grasp pose relative to the AprilTag frame.</input_port>
        </Action>
    </TreeNodesModel>
</root>
